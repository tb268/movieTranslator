# -*- coding: utf-8 -*-
"""soturon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uf_zQ9oNtNpTGEpJY3IX47PiqsjIL4TO
"""

#!python -V

#!ffmpeg -V

#翻訳用(仮)
!pip install googletrans==4.0.0-rc1
from googletrans import Translator

!pip install tesseract
!pip install opencv-python
!apt install tesseract-ocr libtesseract-dev tesseract-ocr-jpn
!pip install pyocr

!pip install ffmpeg-python

#moviepyのインストール
!pip install moviepy

!pip install imageio==2.4.1

import cv2
import os
import numpy as np
import pyocr
import pyocr.builders
from PIL import Image, ImageFont, ImageDraw
import pyocr
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt
#import difflib
#import argparse
import ffmpeg 
from googletrans import Translator
import csv
import string

import moviepy.editor as mp
import math



def recognition_sub(ass,dss):#文字認識
  after=cv2.imread("./before.jpeg")
  before=cv2.imread("./after.jpeg")
  txt_1=recognition(before)
  txt_2=recognition(after)
  return txt_1==txt_2

def comparison(before,after):#前後の画像が一致しているかどうか
  if np.array_equal(before,after):
    return True
  else:
    #print(np.array_equal(before,after))
    #画像をヒストグラム化
    #グレースケース変換
    gray_img_before = cv2.cvtColor(before, cv2.COLOR_BGR2GRAY)
    gray_img_after = cv2.cvtColor(after, cv2.COLOR_BGR2GRAY)
    #単純に画像の引き算
    img_diff = cv2.absdiff(gray_img_before, gray_img_after) 
    #差分画像の２値化（閾値が50）
    ret, img_bin = cv2.threshold(img_diff, 50, 255, cv2.THRESH_OTSU)
    #全体の画素数
    whole_area=before.size
    #白部分の画素数
    white_area=cv2.countNonZero(img_diff)
    # ヒストグラムした画像を比較
    #print(before,after)
    #print(cv2.compareHist(before_hist, after_hist, 0)) 
    if white_area/whole_area>0.02 and white_area/whole_area<0.98:
      return False#画像は異なるもの
    else:
      return True#画像は同じもの

#画像の秒数確認
#def timecheck(frame,video_path2):
    #cap = cv2.VideoCapture(video_path2)                  # 動画を読み込む
def timecheck(frame,cap):
    #video_frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT) # フレーム数を取得する
    video_fps = cap.get(cv2.CAP_PROP_FPS)                 # フレームレートを取得する
    #video_len_sec = video_frame_count / video_fps         # 長さ（秒）を計算する
    a=frame/video_fps#今の秒数
    return a

tr = Translator()
txt = tr.translate("hello", src="en", dest="ja").text
print(txt)

#アルファベットが1文字以上あるならTrue,それ以外False
def onlyalpha(stri):
  x=0
  for i in stri:
    if i in string.ascii_letters:
      x+=1
  print(x)
  if x>0:
    return True
  else:
    return False
#print(onlyalpha("(a)"))



#テキスト化（ブロック版）（完成）
def recognition(time,img):#返すのはtext（）list型

  #グレースケール変換
  #gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
  #差分画像の２値化（閾値が50）
  pil_img = Image.fromarray(img)
  tools = pyocr.get_available_tools()
  tool = tools[0]
  #文字検出の前に四角形検出
  #check_rectangle(img)[1]
  #文字のボックス処理について！！！ここからーーーーーーーーーーーーーーーーーーーーーーーーーーーー
  text = tool.image_to_string(#txt1が画像をテキスト化したもの
      pil_img,
      lang='jpn+eng',
      builder=pyocr.builders.LineBoxBuilder(tesseract_layout=6)
      #builder=pyocr.builders.WordBoxBuilder(tesseract_layout=6)
  )

  #print(text)
  boxdata=[]
  contentdata=[]
  for t in text:
    org=t.position

    #文字列が空白でなく、かつ1文字以下の場合のみ実行ーーーーーーーーーーーーーーーーーーーーーーーーーー
    if not t.content.isspace() and not len(t.content)<0 :
      #翻訳用処理-----------------------------------------------------------------------------------
      #1文字以上英語がある場合、翻訳
      if onlyalpha(t.content):
        tr = Translator()
        en = tr.translate(str(t.content), src="en", dest="ja").text
      else:
        en=str(t.content)
      #----------------------------------------------------------------------------
      boxdata.append([en,t.position])
      contentdata.append(en)
  
    pass
  with open("text.csv","a", newline='') as f:
    #print([str(time),contentdata],file=f)
    print("str(time)")
    print(str(time))
    contentdata.insert(0,str(time))
    print("contentdata")
    print(contentdata)
    writer = csv.writer(f)
    writer.writerow(contentdata)  
  #beforeとafterで2回表示される

  if boxdata==[]:#boxdataがない（字幕がない場合）、からデータを送信
    return ["",((0,0),(0,0))]
  return boxdata#[0]が文字、[1]が位置(x座標、y座標)　(1フレーム内の画像のボックス全ての文字とその位置)
  #文字のボックス処理について！！！ここまでーーーーーーーーーーーーーーーーーーーーーーーーーーーー

#ーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー

#and (not (len(t.content)<2 and t in ["$","@","%","#","(",")","[","]","!","&","?","¥"])

from IPython.core.display import skip_doctest
#メイン処理(ボックス版)
#動画から毎フレームを取り出す
def check(video_path="解説",dir_path="./",isTranslate =True,basename="basename",ext="jpg"):
  try:
    os.remove("text.csv")
  except:
    pass
  try:
    os.remove(os.path.join(*[dir_path, 'output_' + video_path]))
  except:
    pass
  cap_file =cv2.VideoCapture(video_path)#ビデオ取得
  #動画から音声を抽出（入出力確認済み）
  #ーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー
  # 入力 
  stream = ffmpeg.input(video_path)
  # 出力 
  stream_audio = stream.audio
  #stream_audio = ffmpeg.output(stream.audio,"test.mp3")
  #ーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー
  video_frame_count = cap_file.get(cv2.CAP_PROP_FRAME_COUNT) # フレーム数を取得する
  read_fps= cap_file.get(cv2.CAP_PROP_FPS) 
  #ひらけなかった時の処理ーーーーーーーーーーーー
  if not cap_file.isOpened():
    print("ファイルをひらけませんでした.指定したファイルパスが存在するか確認してください")
    return
  #ーーーーーーーーーーーーーーーーーーーーーーーー
  os.makedirs(dir_path, exist_ok=True)#フォルダの作成
  #base_path = os.path.join(dir_path, basename)#読み込み画像の名前
  fps = 5 #希望のfps（動画1秒あたり何枚切り出して処理したいか）
  thresh = read_fps / fps #フレーム何枚につき1枚処理するか
  digit = len(str(int(cap_file.get(cv2.CAP_PROP_FRAME_COUNT))))
  k=0
  n = 0
  before_frame=0
  message=[]
  position=[]
  nowbox=[]
  beforebox=[]
  beforetime=0
  nowtime=0
  frame_counter=0
  while True:
    
      ret, frame = cap_file.read()#retがbool（読み込めたか）、frameが画像
      time = n / int(read_fps/1)        # 抽出したフレームの動画内経過時間
      print("time")
      print(time)
      if n==0:#0フレーム目の時
        #cv2.imwrite('{}_{}.{}'.format(base_path, str(n).zfill(digit), ext), frame)
        beforeimg=frame
        nowimg=frame
        #テキスト検出
        #テスト中
        #beforebox=recognition(time,beforeimg)
        beforebox=nowbox
        nowbox=recognition(time,nowimg)#nowbox内は[文字、[（左上x,y)、(右上x,y）]]
        

      if ret:
        #------------------------
        frame_counter += 1
        print(frame_counter)
        if (frame_counter >= thresh): #フレームカウントがthreshを超えたら処理する
          print("処理")
          #------------------------
          #cv2.imwrite('{}_{}.{}'.format(base_path, str(n).zfill(digit), ext), frame)
          beforeimg=nowimg#前のフレームの画像を入れる
          nowimg=frame#今のフレームの画像を入れる
          print("フレームは"+str(n)+"/"+str(video_frame_count))
          #前フレームと異なっていた場合（全一致でない場合）
          #テスト
          #beforetime =timecheck(before_frame,cap_file)
          nowtime=timecheck(n,cap_file)
          if comparison(nowimg,beforeimg) and not n==0 and not n==video_frame_count-1:#前フレームと異なっていない場合（全一致の場合）
            beforebox=nowbox
            pass
          else:#前フレームと画像が異なる場合
            #テキスト化処理ーーーーーーーーー            
            for i in nowbox:
              try:  
                if not len(i[0])==0 or not i[0]:
                  message.append([[i[0],[i[1][0][0],i[1][0][1]],[i[1][1][0],i[1][1][1]]],beforetime,nowtime])
              except:
                print("エラー1")
                pass
            #前の画像をnowboxにしていいかテスト
            #beforebox=recognition(beforeimg)
            beforebox=nowbox
            nowbox=recognition(time,nowimg)#nowbox内は[文字、[（左上x,y)、(右上x,y）]]
            beforetime=nowtime
            print("画像変更がありました")
            k+=1
          
          frame_counter = 0 #フレームカウントを０に戻す
          

        #print("現在のフレームのbox座標とテキスト")
        #print(nowbox)
        
        try:
          #フレームの画像保存(直接必要はなし)
          position=nowbox[n][1]
          #これ
          #テロップ挿入
          before_frame=n
        except IndexError:
          print("インデックスエラー")
          pass

        n += 1
      else:#最後のフレームになったら終了処理に移行ーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー
        print("開始！！！！！")
        print(k)
        #以下字幕入れる処理
        dir = './'               # 動画が保存されているディレクトリ
        path = video_path   # ファイル名
        step = 1    #ステップは1で固定
        print("メッセージは"+str(message), end='')
        print(message)
        m_slice(path, dir, step, message)
        #動画と音声と合成ーーーーーーーーーーーーーーーーーーーーーーーーーーーー
        out_path = os.path.join(*[dir, 'out_' + path])
        out_path2 = os.path.join(*[dir, 'output_' + path])
        v=ffmpeg.input(out_path) 
        video = ffmpeg.output(stream_audio, v,out_path2)
        
        # 実行
        ffmpeg.run(video)
        os.remove(out_path)
        print(message[0])
        #ーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー
        return

#ーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー
#check("pawapo.mp4")
#メイン処理(ボックス版)ーーーーーーーーーーーーーーーーーーーーーーーーーー

def set_audio(srcfile,imgfile,outfile):
    # Extract audio from input video.                                                                     
    clip_input = mp.VideoFileClip(srcfile)
    clip_input.audio.write_audiofile('audio.mp3')
    # Add audio to output video.                                                                          
    clip = mp.VideoFileClip(imgfile).subclip()
    clip.write_videofile(outfile, audio='audio.mp3')

# 画像に文字を入れる関数（完成！！！！）
def telop(img, messages, W, H):
    print(messages)
    font_path = 'fonts-japanese-gothic.ttf'        # Windowsのフォントファイルへのパス
    font_size =24
    img = Image.fromarray(img)                       # cv2(NumPy)型の画像をPIL型に変換
    draw = ImageDraw.Draw(img)                       # 描画用のDraw関数を用意

    #文字検出の前に四角形検出ーーーーーーーーーーーーーーーーーーー
    #check_rectangle(img)

    # テキストを描画（位置、文章、フォント、文字色(BGR+α)を指定）
    if messages==[]:
      pass
    else:
      for m in messages:
        #w, h = draw.textsize(m[0], font)              # .textsizeで文字列のピクセルサイズを取得

        # テロップの位置positionは画像サイズと文字サイズから決定する
        # 横幅中央、縦は下
        #position = (int((W - w) / 2), int(H - (font_size * 1.5)))
        # 中央揃え
        #position = (int((W - w) / 2), int((H - h) / 2))
        font_size = m[2][1]-m[1][1]                                   # フォントサイズ
        font = ImageFont.truetype(font_path, font_size)  # PILでフォントを定義
        #print("font_size")
        #print(m[2][1]-m[1][1] )
        #四角形と字幕をかく
        draw.rectangle((m[1][0],m[1][1], m[2][0],m[2][1]), fill=(255, 255, 255), outline=(0, 0, 0))
        draw.text([m[1][0],m[1][1]],
                  m[0],
                  font=font,
                  fill=(255,255,0,0))
        pass

    # PIL型の画像をcv2(NumPy)型に変換
    img = np.array(img)
    return img

# 動画を読み込み1フレームずつ画像処理をする関数
def m_slice(path, dir, step, message):
    in_path = os.path.join(*[dir, path])                # 読み込みパスを作成
    out_path = os.path.join(*[dir, 'out_' + path])      # 書き込みパスを作成
    movie = cv2.VideoCapture(in_path)                   # 動画の読み込み
    Fs = int(movie.get(cv2.CAP_PROP_FRAME_COUNT))       # 動画の全フレーム数を計算
    fps = movie.get(cv2.CAP_PROP_FPS)                   # 動画のFPS（フレームレート：フレーム毎秒）を取得
    W = int(movie.get(cv2.CAP_PROP_FRAME_WIDTH))        # 動画の横幅を取得
    H = int(movie.get(cv2.CAP_PROP_FRAME_HEIGHT))       # 動画の縦幅を取得
    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v') # 動画保存時のfourcc設定（mp4用）

    # 動画の仕様（ファイル名、fourcc, FPS, サイズ）
    video = cv2.VideoWriter(out_path, fourcc, int(fps / step), (W, H))
    ext_index = np.arange(0, Fs, step)  # 動画から静止画（フレーム）を抽出する間隔

    j = 0                               # message配列から文章と時間を抜き出す指標番号
    section = message[j]                # フレームに書き込む文章と時間の初期値
    for i in range(Fs):                 # フレームサイズ分のループを回す
        s=[]
        flag, frame = movie.read()      # 動画から1フレーム読み込む(frameが画像データ)
        check = i == ext_index          # 現在のフレーム番号iが、抽出する指標番号と一致するかチェックする
        time = i / int(fps/step)        # 抽出したフレームの動画内経過時間
        
        if flag == True: # フレームを取得できた時だけこの処理をする
            # もしi番目のフレームが静止画を抽出するものであれば、ファイル名を付けて保存する
            if True in check:
                # ここから動画フレーム処理と動画保存---------------------------------------------------------------------
                # messageの数の分繰り返し
                for sect in message:
                  # 抽出したフレームの再生時間がテロップを入れる時間範囲に入っていれば文字入れする
                  if sect[1] <= time <= sect[2]:
                    s.append(sect[0])
                  # 再生時間がテロップ入れ開始時間より小さければ待機する
                  elif sect[1] > time:
                    continue
                    pass
                
                print(i)
                frame = telop(frame, s, W, H)  # テロップを入れる関数を実行
                video.write(frame)                          # 動画を1フレームずつ保存する
            # ここまでが動画フレーム処理と保存---------------------------------------------------------------------
            # i番目のフレームが静止画を抽出しないものであれば、何も処理をしない
            else:
                pass
        else:
            pass
    return

# pt0-> pt1およびpt0-> pt2からの
# ベクトル間の角度の余弦(コサイン)を算出
def angle(pt1, pt2, pt0) -> float:
    dx1 = float(pt1[0,0] - pt0[0,0])
    dy1 = float(pt1[0,1] - pt0[0,1])
    dx2 = float(pt2[0,0] - pt0[0,0])
    dy2 = float(pt2[0,1] - pt0[0,1])
    v = math.sqrt((dx1*dx1 + dy1*dy1)*(dx2*dx2 + dy2*dy2) )
    return (dx1*dx2 + dy1*dy2)/ v

# 画像上の四角形を検出
def findSquares(bin_image, image, cond_area = 1000):
    # 輪郭取得
    n=0
    rcnts=[]
    contours, _ = cv2.findContours(bin_image, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
    for i, cnt in enumerate(contours):
        # 輪郭の周囲に比例する精度で輪郭を近似する
        arclen = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, arclen*0.02, True)

        #四角形の輪郭は、近似後に4つの頂点があります。
        #比較的広い領域が凸状になります。

        # 凸性の確認 
        area = abs(cv2.contourArea(approx))
        if approx.shape[0] == 4 and area > cond_area and cv2.isContourConvex(approx) :
            maxCosine = 0

            for j in range(2, 5):
                # 辺間の角度の最大コサインを算出
                cosine = abs(angle(approx[j%4], approx[j-2], approx[j-1]))
                maxCosine = max(maxCosine, cosine)

            # すべての角度の余弦定理が小さい場合
            #（すべての角度は約90度です）次に、quandrangeを書き込みます
            # 結果のシーケンスへの頂点
            if maxCosine < 0.3 :
                # 四角判定!!
                n+=1
                rcnt = approx.reshape(-1,2)
                #rcntは四角形の座標
                cv2.polylines(image, [rcnt], True, (0,0,255), thickness=2, lineType=cv2.LINE_8)
                rcnts.append([rcnt])
    print(n)#四角形の数
    print(rcnts)#各四角形の4点のリスト
    return image,rcnts

def check_rectangle(image):
    #image = cv2.imread(image_path, cv2.IMREAD_COLOR)
    image=np.array(image)
    if image is None :
        exit(1)
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, bw = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    rimage = findSquares(bw, image)[0]
    rcnts=findSquares(bw, image)[1]
    plt.imshow(rimage)  #結果表示
    c = cv2.waitKey()
    return 0 ,rcnts;

import sys

args = sys.argv
if args[2]=="T":
  TF=True
else:
  TF=False

#実行用、isTranslateは翻訳するかどうか
check(args[1],isTranslate=TF)



